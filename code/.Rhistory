stri_escape_unicode("\u00b5")
stri_unescape_unicode("\u00b5")
stri_rand_lipsum(3)
stri_rand_shuffle("hello")
stri_rand_strings(100, 10, pattern = "[berlin]")
stri_unescape_unicode("\U0001f600")
stri_unescape_unicode("\u0001f600")
stri_escape_unicode("\U0001f600")
library(utf8)
utf8_print(intToUtf8(0x1f600 + 0:79)) # truncates to line width
stri_rand_strings(100, 10, pattern = "[berlin]")
## load packages -----------------
library(stringr)
## string manipulation ----------
example.obj <- "1. A small sentence. - 2. Another tiny sentence."
example.obj
# locate
str_locate(example.obj, "tiny")
# substring extraction
str_sub(example.obj, start = 35, end = 38)
# replacement
str_sub(example.obj, 35, 38) <- "huge"
str_replace(example.obj, pattern = "huge", replacement = "giant")
# splitting
str_split(example.obj, "-") %>% unlist
# manipulate multiple elements; example
(char.vec <- c("this", "and this", "and that"))
# detection
str_detect(char.vec, "this")
# keep strings matching a pattern
str_subset(char.vec, "this") # wrapper around x[str_detect(x, pattern)]
# counting
str_count(char.vec, "this")
str_count(char.vec, "\\w+")
str_length(char.vec)
# duplication
str_dup(char.vec, 3)
# padding and trimming
length.char.vec <- str_length(char.vec)
char.vec <- str_pad(char.vec, width = max(length.char.vec), side = "both", pad = " ")
char.vec
str_trim(char.vec)
# joining
str_c("text", "manipulation", sep = " ")
str_c(char.vec, collapse = "\n") %>% cat
str_c("text", c("manipulation", "basics"), sep = " ")
# approximate matching
agrepl("Donald Trump", "Donald Drumpf", max.distance = list(all = 3))
agrepl("Donald Trump", "Barack Obama", max.distance = list(all = 3))
# load packages
library(stringr)
library(rvest)
# query your locale for individual categories
localeCategories <- c("LC_COLLATE","LC_CTYPE","LC_MONETARY","LC_NUMERIC","LC_TIME")
# sample from the list of available conversions
(encodings <- length(iconvlist()))
sample(iconvlist(), 10)
# an example string
small.frogs <- "Små grodorna, små grodorna är lustiga att se."
small.frogs
library(tidyverse)
library(rvest)
library(pdftools)
# devtools::install_github("hrbrmstr/nominatim")
library(nominatim)
## step 1: inspect page
url <- "http://ajps.org/list-of-reviewers/"
browseURL(url)
## step 2: retrieve pdfs
# get page
content <- read_html(url)
# get anchor (<a href=...>) nodes via xpath
anchors <- html_nodes(content, xpath = "//a")
# get value of anchors' href attribute
hrefs <- html_attr(anchors, "href")
# filter links to pdfs
pdfs <- hrefs[ str_detect(basename(hrefs), ".*\\d{4}.*pdf") ]
pdfs
# define names for pdfs on disk
pdf_names <- str_extract(basename(pdfs), "\\d{4}") %>% paste0("reviewers", ., ".pdf")
pdf_names
# download pdfs
dir.create("ajps-reviewers")
for(i in seq_along(pdfs)) {
download.file(pdfs[i], paste0("ajps-reviewers/", pdf_names[i]), mode="wb")
}
## step 3: import pdf
rev_raw <- pdftools::pdf_text("ajps-reviewers/reviewers2015.pdf")
class(rev_raw)
rev_raw[1]
## step 4: tidy data
rev_all <- rev_raw %>% str_split("\\n") %>% unlist
surname <- str_extract(rev_all, "[[:alpha:]-]+")
prename <- str_extract(rev_all, " [.[:alpha:]]+")
rev_df <- data.frame(raw = rev_all, surname = surname, prename = prename, stringsAsFactors = F)
rev_df$institution <- NA
for(i in 1:nrow(rev_df)) {
rev_df$institution[i] <- rev_df$raw[i] %>% str_replace(rev_df$surname[i], "") %>% str_replace(rev_df$prename[i], "") %>% str_trim()
}
rev_df <- rev_df[-c(1,2),]
rev_df <- rev_df[!is.na(rev_df$surname),]
head(rev_df)
View(rev_df)
install.packages("rnaturalearth")
regex <- ".*"
string <- c("1. This is an example string by", "2. Eddie (born 1961 in München)", "!§%$&/)(}")
regex <- ".*"
string <- c("1. This is an example string by", "2. Eddie (born 1961 in München)", "!§%$&/)(}")
str_extract_all(string, regex)
string <- c("Look at this fancy example string! I can even do 😀😁😂", "2. Eddie (born 1961 in München)", "!§%$&/)(}")
str_extract_all(string, regex)
string <- c("Look at this fancy example string!", "2. Eddie (born 1961 in München)", "!§%$&/)(}")
str_extract_all(string, regex)
library(tidyverse)
library(rvest)
library(stringr)
# devtools::install_github("hrbrmstr/nominatim")
library(nominatim)
library(sf)
library(rnaturalearth)
browseURL("http://www.biermap24.de/brauereiliste.php")
# set temporary working directory
tempwd <- ("data/breweriesGermany")
dir.create(tempwd)
# set temporary working directory
tempwd <- ("breweriesGermany")
dir.create(tempwd)
setwd(tempwd)
## step 1: fetch list of cities with breweries
url <- "http://www.biermap24.de/brauereiliste.php"
content <- read_html(url)
anchors <- html_nodes(content, xpath = "//tr/td[2]")
cities <- html_text(anchors)
cities
cities <- str_trim(cities)
cities <- cities[str_detect(cities, "^[[:upper:]]+.")]
cities <- cities[6:length(cities)]
length(cities)
length(unique(cities))
sort(table(cities))
unique_cities <- unique(cities)
pos <- data.frame(lon = NA, lat = NA)
setwd("../")
## step 2: geocode cities
# get free key for mapquest API at browseURL("https://developer.mapquest.com/")
load("/Users/s.munzert/rkeys.RDa") # import API key (or paste it here in openstreetmap object)
pos <- data.frame(lon = NA, lat = NA)
if (!file.exists("geocodes_cities.RData")){
for (i in 1:length(unique_cities)) {
pos[i,] <- try(nominatim::osm_search(unique_cities[i], country_codes = "de", key = openstreetmap) %>% dplyr::select(lon, lat))
}
pos$city <- unique_cities
pos <- filter(pos, !str_detect(lon, "Error"))
pos$lon <- as.numeric(pos$lon)
pos$lat <- as.numeric(pos$lat)
save(pos, file="geocodes_breweriers.RData")
} else {
load("geocodes_breweries.RData")
}
dir()
head(pos)
load("geocodes_breweries.RData")
if (!file.exists("geocodes_breweries.RData")){
for (i in 1:length(unique_cities)) {
pos[i,] <- try(nominatim::osm_search(unique_cities[i], country_codes = "de", key = openstreetmap) %>% dplyr::select(lon, lat))
}
pos$city <- unique_cities
pos <- filter(pos, !str_detect(lon, "Error"))
pos$lon <- as.numeric(pos$lon)
pos$lat <- as.numeric(pos$lat)
save(pos, file="geocodes_breweriers.RData")
} else {
load("geocodes_breweries.RData")
}
head(pos)
## step 3: plot breweries of Germany
pos <- filter(pos, lon >= 6, lon <= 15, lat >= 47, lat <= 55)
worldmap <- rnaturalearth::ne_countries(scale = 'medium', type = 'map_units', returnclass = 'sf')
install.packages("rnaturalearthdata")
## step 3: plot breweries of Germany
pos <- filter(pos, lon >= 6, lon <= 15, lat >= 47, lat <= 55)
worldmap <- rnaturalearth::ne_countries(scale = 'medium', type = 'map_units', returnclass = 'sf')
install.packages("rgeos")
## step 3: plot breweries of Germany
pos <- filter(pos, lon >= 6, lon <= 15, lat >= 47, lat <= 55)
worldmap <- rnaturalearth::ne_countries(scale = 'medium', type = 'map_units', returnclass = 'sf')
germany <- worldmap[worldmap$name == 'Germany',]
ggplot() + geom_sf(data = germany) + theme_bw() + geom_point(aes(lon,lat), data = pos, size = .5, color = "red")
View(mtcars)
# enter your R code here
sum(1, 2)
sum(1, 2)
raw.data <- "555-1239Moe Szyslak(636) 555-0113Burns, C. Montgomery555-6542Rev. Timothy Lovejoy555 8904Ned Flanders636-555-3226Simpson, Homer5553642Dr. Julius Hibbert"
phone <- unlist(str_extract_all(raw.data, "\\(?(\\d{3})?\\)?(-| )?\\d{3}(-| )?\\d{4}"))
library(tidyverse)
library(rvest)
raw.data <- "555-1239Moe Szyslak(636) 555-0113Burns, C. Montgomery555-6542Rev. Timothy Lovejoy555 8904Ned Flanders636-555-3226Simpson, Homer5553642Dr. Julius Hibbert"
phone <- unlist(str_extract_all(raw.data, "\\(?(\\d{3})?\\)?(-| )?\\d{3}(-| )?\\d{4}"))
phone
# diagnosis
str_extract("+test" , "\\+.+")
# diagnosis
str_extract("+test" , "\\+.+")
# positive/negative lookahead/lookbehind assertions
example.obj <- "1. A small sentence. - 2. Another tiny sentence."
# positive/negative lookahead/lookbehind assertions
example.obj <- "1. A small sentence. - 2. Another tiny sentence."
unlist(str_extract_all(example.obj, "(?<=2. ).+")) # positive lookbehind: (?<=...)
unlist(str_extract_all(example.obj, ".+(?=2)")) # positive lookahead (?=...)
# solution
str_extract("+test" , "(?<=\\+).+")
str_extract_all("Hello, my name is Simon" , "[[a-z]., ]")
str_extract_all("Hello, my name is Simon" , "[^[., ]]")
str_extract_all("Hello, my name is Simon" , "[^[., ]]")
str_extract_all("Hello, my name is Simon" , "[^., ]") # equivalent
## load packages
library(tidyverse)
library(rvest)
library(stringr)
# devtools::install_github("hrbrmstr/nominatim")
library(nominatim)
library(sf)
library(rnaturalearth)
## step 1: fetch list of cities with breweries
url <- "http://www.biermap24.de/brauereiliste.php"
content <- read_html(url)
anchors <- html_nodes(content, xpath = "//tr/td[3]")
cities <- html_text(anchors)
cities
cities <- str_trim(cities)
length(cities)
length(unique(cities))
sort(table(cities))
unique_cities <- unique(cities)
coords_df <- data.frame(city = NA, lon = NA, lat = NA, stringsAsFactors = FALSE)
if (!file.exists("coords_breweries.RData")){
for (i in 1:length(unique_cities)) {
coords <- try(nominatim::osm_search(unique_cities[i], country_codes = "de", key = openstreetmap))
coords_df$city <- unique_cities[i]
if(nrow(coords) > 0) {
coords_df$lon <- as.numeric(coords$lon)
coords_df$lat <- as.numeric(coords$lat)
}else{
coords_df$lon <- NA
coords_df$lat <- NA
}
}
save(coords_df, file="coords_breweries.RData")
} else {
load("coords_breweries.RData")
}
## step 2: geocode cities
# get free key for mapquest API at browseURL("https://developer.mapquest.com/")
load("/Users/simonmunzert/rkeys.RDa") # import API key (or paste it here in openstreetmap object)
coords_df <- data.frame(city = NA, lon = NA, lat = NA, stringsAsFactors = FALSE)
if (!file.exists("coords_breweries.RData")){
for (i in 1:length(unique_cities)) {
coords <- try(nominatim::osm_search(unique_cities[i], country_codes = "de", key = openstreetmap))
coords_df$city <- unique_cities[i]
if(nrow(coords) > 0) {
coords_df$lon <- as.numeric(coords$lon)
coords_df$lat <- as.numeric(coords$lat)
}else{
coords_df$lon <- NA
coords_df$lat <- NA
}
}
save(coords_df, file="coords_breweries.RData")
} else {
load("coords_breweries.RData")
}
n_cities <- length(unique_cities)
coords_df <- data.frame(city = rep(NA, n_cities), lon = rep(NA, n_cities), lat = rep(NA, n_cities), stringsAsFactors = FALSE)
for (i in 1:10) {
coords <- try(nominatim::osm_search(unique_cities[i], country_codes = "de", key = openstreetmap))
coords_df$city[i] <- unique_cities[i]
if(nrow(coords) > 0) {
coords_df$lon[i] <- as.numeric(coords$lon)
coords_df$lat[i] <- as.numeric(coords$lat)
}else{
coords_df$lon <- NA
coords_df$lat <- NA
}
}
coords_df
n_cities <- length(unique_cities)
coords_df <- data.frame(city = rep(NA, n_cities), lon = rep(NA, n_cities), lat = rep(NA, n_cities), stringsAsFactors = FALSE)
n_cities <- length(unique_cities)
coords_df <- data.frame(city = rep(NA, n_cities), lon = rep(NA, n_cities), lat = rep(NA, n_cities), stringsAsFactors = FALSE)
if (!file.exists("coords_breweries.RData")){
for (i in 1:n_cities) {
coords <- try(nominatim::osm_search(unique_cities[i], country_codes = "de", key = openstreetmap))
coords_df$city[i] <- unique_cities[i]
if(nrow(coords) > 0) {
coords_df$lon[i] <- as.numeric(coords$lon)
coords_df$lat[i] <- as.numeric(coords$lat)
}else{
coords_df$lon <- NA
coords_df$lat <- NA
}
}
save(coords_df, file="coords_breweries.RData")
} else {
load("coords_breweries.RData")
}
save(coords_df, file="coords_breweries.RData")
View(coords_df)
coords <- try(nominatim::osm_search(unique_cities[i], country_codes = "de", key = openstreetmap))
coords
coords_df$lat
coords$lat
nrow(coords)
coords
n_cities <- length(unique_cities)
coords_df <- data.frame(city = rep(NA, n_cities), lon = rep(NA, n_cities), lat = rep(NA, n_cities), stringsAsFactors = FALSE)
for (i in 1:n_cities) {
coords <- try(nominatim::osm_search(unique_cities[i], country_codes = "de", key = openstreetmap))
coords_df$city[i] <- unique_cities[i]
if(nrow(coords) > 0) {
coords_df$lon[i] <- as.numeric(coords$lon)
coords_df$lat[i] <- as.numeric(coords$lat)
}else{
coords_df$lon[i] <- NA
coords_df$lat[i] <- NA
}
}
save(coords_df, file="coords_breweries.RData")
View(coords_df)
head(coords_df)
## step 3: plot breweries of Germany
pos <- filter(pos, lon >= 6, lon <= 15, lat >= 47, lat <= 55)
## step 3: plot breweries of Germany
pos <- filter(coords_df, lon >= 6, lon <= 15, lat >= 47, lat <= 55)
worldmap <- rnaturalearth::ne_countries(scale = 'medium', type = 'map_units', returnclass = 'sf')
germany <- worldmap[worldmap$name == 'Germany',]
ggplot() + geom_sf(data = germany) + theme_bw() + geom_point(aes(lon,lat), data = pos, size = .5, color = "red")
# load RSelenium
library(tidyverse)
library(rvest)
library(RSelenium)
# initiate Selenium driver
rD <- rsDriver(browser = "chrome")
remDr <- rD[["client"]]
# start browser, navigate to page
url <- "https://www.imdb.com/search/title"
remDr$navigate(url)
# enter keyword in title field
xpath <- '//*[@id="main"]/div[1]/div[2]/input'
titleElem <- remDr$findElement(using = 'xpath', value = xpath)
titleElem$sendKeysToElement(list("data")) # enter key word
# close connection
remDr$closeServer()
library(tidyverse)
library(rvest)
library(stringr)
# devtools::install_github("hrbrmstr/nominatim")
library(nominatim)
library(sf)
library(rnaturalearth)
browseURL("http://www.biermap24.de/brauereiliste.php")
## step 1: fetch list of cities with breweries
url <- "http://www.biermap24.de/brauereiliste.php"
content <- read_html(url)
anchors <- html_nodes(content, xpath = "//tr/td[3]")
cities <- html_text(anchors)
cities
cities <- str_trim(cities)
length(cities)
length(unique(cities))
sort(table(cities))
## step 1: fetch list of cities with breweries
url <- "http://www.biermap24.de/brauereiliste.php"
content <- read_html(url)
anchors <- html_nodes(content, xpath = "//tr/td[3]")
cities <- html_text(anchors)
cities
cities <- str_trim(cities)
length(cities)
length(unique(cities))
sort(table(cities))
## step 2: geocode cities
# get free key for mapquest API at browseURL("https://developer.mapquest.com/")
load("/Users/s.munzrt/rkeys.RDa") # import API key (or paste it here in openstreetmap object)
## step 2: geocode cities
# get free key for mapquest API at browseURL("https://developer.mapquest.com/")
load("/Users/s.munzert/rkeys.RDa") # import API key (or paste it here in openstreetmap object)
n_cities <- length(unique_cities)
coords_df <- data.frame(city = rep(NA, n_cities), lon = rep(NA, n_cities), lat = rep(NA, n_cities), stringsAsFactors = FALSE)
n_cities <- length(unique_cities)
unique_cities <- unique(cities)
n_cities <- length(unique_cities)
n_cities <- length(unique_cities)
coords_df <- data.frame(city = rep(NA, n_cities), lon = rep(NA, n_cities), lat = rep(NA, n_cities), stringsAsFactors = FALSE)
if (!file.exists("coords_breweries.RData")){
for (i in 1:n_cities) {
coords <- try(nominatim::osm_search(unique_cities[i], country_codes = "de", key = openstreetmap))
coords_df$city[i] <- unique_cities[i]
if(nrow(coords) > 0) {
coords_df$lon[i] <- as.numeric(coords$lon)
coords_df$lat[i] <- as.numeric(coords$lat)
}else{
coords_df$lon[i] <- NA
coords_df$lat[i] <- NA
}
}
save(coords_df, file="coords_breweries.RData")
} else {
load("coords_breweries.RData")
}
head(coords_df)
## step 3: plot breweries of Germany
pos <- filter(coords_df, lon >= 6, lon <= 15, lat >= 47, lat <= 55)
worldmap <- rnaturalearth::ne_countries(scale = 'medium', type = 'map_units', returnclass = 'sf')
install.packages("rnaturalearthdata")
## step 3: plot breweries of Germany
pos <- filter(coords_df, lon >= 6, lon <= 15, lat >= 47, lat <= 55)
worldmap <- rnaturalearth::ne_countries(scale = 'medium', type = 'map_units', returnclass = 'sf')
germany <- worldmap[worldmap$name == 'Germany',]
ggplot() + geom_sf(data = germany) + theme_bw() + geom_point(aes(lon,lat), data = pos, size = .5, color = "red")
url <- "https://en.wikipedia.org/wiki/List_of_female_scientists_in_the_20th_century"
browseURL(url)
# load packages
library(tidyverse)
library(rvest)
# parse page
url <- "https://en.wikipedia.org/wiki/List_of_state_leaders_in_2020"
url_parsed <- read_html(url)
# extract country nodes
country_nodes <- html_nodes(url_parsed, xpath = "//li[./child::b]")
country_text <- html_nodes(country_nodes, xpath = "./*[1]//a[1]")%>% html_text() # does not provide full list, because not all countries are in anchor tags
country_text <- html_nodes(country_nodes, xpath = "./*[1]")%>% html_text() %>% str_trim
country_text
# inspect structure of first country node
xml_structure(country_nodes[[1]])
# extract text for first two leaders
leader_1_text <- sapply(country_nodes, function(x) {html_nodes(x, xpath = "./ul/li[1]") %>% html_text()})
leader_1_text[lengths(leader_1_text) == 0] <- ""
leader_2_text <- sapply(country_nodes, function(x) {html_nodes(x, xpath = "./ul/li[2]") %>% html_text()})
leader_2_text[lengths(leader_2_text) == 0] <- ""
# extract links for first two leaders
leader_1_link <- sapply(country_nodes, function(x) {html_nodes(x, xpath = "./ul/li[1]/a[1]") %>% html_attr("href")})
leader_1_link[lengths(leader_1_link) == 0] <- ""
leader_2_link <- sapply(country_nodes, function(x) {html_nodes(x, xpath = "./ul/li[2]/a[1]") %>% html_attr("href")})
leader_2_link[lengths(leader_2_link) == 0] <- ""
# compile data frame
dat <- data.frame(country = country_text,
leader_1 = unlist(leader_1_text),
leader_1_link = unlist(leader_1_link),
leader_2 = unlist(leader_2_text),
leader_2_link = unlist(leader_2_link),
stringsAsFactors = FALSE)
View(dat)
# load RSelenium
library(tidyverse)
library(rvest)
library(RSelenium)
# initiate Selenium driver
rD <- rsDriver(browser = "chrome")
# close connection
remDr$closeServer()
# initiate Selenium driver
rD <- rsDriver(browser = "chrome")
remDr <- rD[["client"]]
# start browser, navigate to page
url <- "https://www.imdb.com/search/title"
remDr$navigate(url)
# load RSelenium
library(tidyverse)
library(rvest)
library(RSelenium)
# initiate Selenium driver
rD <- rsDriver(browser = "chrome")
remDr <- rD[["client"]]
# start browser, navigate to page
url <- "https://www.imdb.com/search/title"
remDr$navigate(url)
# enter keyword in title field
xpath <- '//*[@id="main"]/div[1]/div[2]/input'
titleElem <- remDr$findElement(using = 'xpath', value = xpath)
titleElem$sendKeysToElement(list("data")) # enter key word
# select requested title data
xpath <- '//*[@id="main"]/div[8]/div[2]/select/option[7]' # plot
titledatElem1 <- remDr$findElement(using = 'xpath', value = xpath)
titledatElem1$clickElement() # click on list element
xpath <- '//*[@id="main"]/div[8]/div[2]/select/option[10]' # technical info
titledatElem2 <- remDr$findElement(using = 'xpath', value = xpath)
titledatElem2$clickElement() # click on list element
# scroll to end of page (just for fun)
webElem <- remDr$findElement("css", "body")
webElem$sendKeysToElement(list(key = "end"))
# click on search button
xpath <- '//*[@id="main"]/p[3]/button'
searchElem <- remDr$findElement(using = 'xpath', value = xpath)
searchElem$clickElement() # click on button
# store index page
output <- remDr$getPageSource(header = TRUE)
write(output[[1]], file = "data/imdb-data-movies.html")
# close connection
remDr$closeServer()
# parse html
content <- read_html("data/imdb-data-movies.html")
titles <- html_nodes(content, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "lister-item-header", " " ))]//a') %>% html_text
head(titles)
write(output[[1]], file = "imdb-data-movies.html")
dir.create("data")
write(output[[1]], file = "data/imdb-data-movies.html")
# close connection
remDr$closeServer()
# parse html
content <- read_html("data/imdb-data-movies.html")
titles <- html_nodes(content, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "lister-item-header", " " ))]//a') %>% html_text
head(titles)
